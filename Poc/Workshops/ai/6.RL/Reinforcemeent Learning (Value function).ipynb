{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résoud *L-Antique Maze* et apprend les bases du reinforcement learning !\n",
    "```\n",
    "     \n",
    "                                           ..,,;;;;;;,,,,\n",
    "                                     .,;'';;,..,;;;,,,,,.''';;,..\n",
    "                                  ,,''                    '';;;;,;''\n",
    "                                 ;'    ,;@@;'  ,@@;, @@, ';;;@@;,;';.\n",
    "                                ''  ,;@@@@@'  ;@@@@; ''    ;;@@@@@;;;;\n",
    "                                   ;;@@@@@;    '''     .,,;;;@@@@@@@;;;\n",
    "                                  ;;@@@@@@;           , ';;;@@@@@@@@;;;.\n",
    "                                   '';@@@@@,.  ,   .   ',;;;@@@@@@;;;;;;\n",
    "                                      .   '';;;;;;;;;,;;;;@@@@@;;' ,.:;'\n",
    "                                        ''..,,     ''''    '  .,;'\n",
    "                                             ''''''::''''''''\n",
    "                                                                 ,;\n",
    "                                {L-Antique}                     .;;\n",
    "                                                               ,;;;\n",
    "                                Author Slo - Slohan.S        ,;;;;:\n",
    "                                                          ,;@@   .;\n",
    "                                                         ;;@@'  ,;\n",
    "                                                         ';;, ,;'        [17/03/2020]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour la suite de ce Workshop j'utiliserais quelques abréviations : \n",
    "\n",
    ">RL \n",
    ": *Reinforcement Learning*   \n",
    ">DL \n",
    ": *Deep Learning*   \n",
    ">MDP \n",
    ": *Markov Decision Process* \n",
    "\n",
    "  \n",
    "\n",
    "Le Reinforcement Learning est une sous-couche du machine Learning, en matière d’apprentissage automatisé, on oppose très fréquemment l'apprentissage supervisé et l'apprentissage non supervisé. Le RL lui fait parti du troisième camp qui se situe entre le supervisé et non-supervisé. D'un côté, il utilise des éléments du supervisé tel que les réseaux de neurones du Deep Learning pour apprendre à partir de données. Et d'un autre côté, il n'utilise aucune donnée labellisée pour faire cet apprentissage, l'apprentissage à partir des données se fait donc d'une manière différente. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby_img](./img/supervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si je devais vous donner une représentation abstraite du RL, je vous dirais ceci :  \n",
    "\n",
    "> Le reinforcement Learning se compose d'un **agent**, d'un **environnement** et de **récompenses**. Prenons un exemple simple. Imaginons un bébé dans une maison avec ses parents. Le bébé représenterait l'agent et la maison et tout ce qui est autour, tel que les parents, représenteraient l'environnement. Le bébé possède un **cerveau vierge**, il ne connaît rien, mais possède 5 sens (la vue, l'ouïe, le touché, ...) pour lui permettre **d'observer** l'environnement (il est important de noter que l'agent ne peut avoir qu'une observation de l'environnement et pas un aperçu total à chaque fois.). Il va prendre des **actions** aléatoires et attendre les récompenses venant de l'environnement. S'il touche du feu, il aura **mal** ce qui équivaut à une mauvaise récompense. S'il se met debout et que ses parent **applaudissent**, ce sera une récompense positive pour lui. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby_img](./img/baby_r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fondation théorique du Reinforcement Learning \n",
    "\n",
    "\n",
    "Vous avez eu un aperçu de ce qu'est le Reinforcement Learning. Maintenant nous allons voir les fondations théoriques qui se cache derrière et qui vont vous permettre de comprendre en détails cette technique. Dans cette section, nous allons voir ce qu'est un processus de décision markovien. Cela représente les fondamentaux et la théorie sur quoi le RL se tient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (MDP) \n",
    "\n",
    "Avant de couvrir le processus de décision markovien complet nous allons commencer par le cas le plus simple qui est le **Processus de Markov** basique. Puis on étendra ce processus avec les récompenses pour arriver au **processus de récompenses markovien** et enfin, on finira par rajouter la dernière enveloppe : les actions. Ce qui va nous mener au **Processus de décision markovien**. Ces fondations théoriques sont utilisées dans de nombreux problèmes de computer science et ne vous donneront pas qu'une base sur le Reinforcement Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Markov Process \n",
    "\n",
    "Le processus de Markov (aussi appelé Chaîne de Markov) est un processus dans lequel vous ne pouvez qu'observer. Imaginez que vous ayez un système devant vous et que vous l'observiez en ayant aucune influence sur celui-ci. Ce que vous observez est appelé **l'état du système** et le système peu passer d'un état à un autre en fonction de certaines règles. Encore une fois, je le répète vous n'êtes qu'un observateur.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple est toujours plus parlant. Imaginez donc un robot se déplaçant dans un labyrinthe de 9 cases. Vous êtes un observateur qui observe le déplacement du robot à chaque instant t. Le robot et le labyrinthe représentent le système et l'état du système représente la position du robot dans le labyrinthe à un instant t. Le robot bouge à chaque instant de case en case (et donc d'état en état) en suivant des règles mais vous n'avez aucune influence sur le déplacement d'un état t à un état t+1.  \n",
    "\n",
    "\n",
    "Tous les états possibles d'un système sont appelés **State Space**. Dans l'exemple que nous avons pris le *State Space* est de 9 car il y a 9 cases dans le labyrinthe. Ce *State Space* doit être une donnée finie, mais peut être extrêmement large. Ci-dessous, on a représenté le changement d'état que l'on a observé au cours du temps. Une séquence d'observations au cours du temps est appelée *Chain Of State* et forme ce qu'on appelle un **historique**.  \n",
    "![baby_img](./img/maze.gif)![transition_img](./img/transition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous l'avez vu, on représente souvent le passage d'état (globalement les MDP) par un graphe avec des nodes. \n",
    "\n",
    "La probabilité de passé d'un état à un autre est définie par le système et l'observateur n'a pas accès à ces probabilités exactes. Il observe simplement le système et peu approximer ces probabilités. \n",
    "\n",
    "Plus on a d'observations, plus notre approximation sera proche des probabilités de changement d'état réel. Pour l'exemple, on nous allons dire que nous avons défini les probabilités du système. Ainsi nous y avons accès, mais il est important de distinguer les probabilités réelles des probabilités estimées. Les probabilités réelles sont fixes et ne changent pas. N'oublions pas que le system est immuable, il a ses données et sa dynamique. Nous nous ne faisons que l'observer. \n",
    "\n",
    "Un élément important lié à cela est la **Propriété de Markov**. Pour qu'un système soit appelé un processus de Markov, il faut que les états du système soient distinguables les uns des autres et uniques. Cela permet de ne dépendre que d'un seul et unique état pour prédire l'état suivant du système et donc de ne pas dépendre de tout l'historique quand nous voulons prédire l'état futur du system. \n",
    "\n",
    "> En probabilité, un processus stochastique vérifie la propriété de Markov si et seulement si la distribution conditionnelle de probabilité des états futurs, étant donné les états passés et l'état présent, ne dépend en fait que de l'état présent et non pas des états passés (absence de « mémoire »). Un processus qui possède cette propriété est appelé processus de Markov. Pour de tels processus, la meilleure prévision que l'on puisse faire du futur, connaissant le passé et le présent, est identique à la meilleure prévision qu'on puisse faire du futur, connaissant uniquement le présent : si on connait le présent, la connaissance du passé n'apporte pas d'information supplémentaire utile pour la prédiction du futur.    \n",
    "[*Propriété de Markov Wikipedia*](https://fr.wikipedia.org/wiki/Propri%C3%A9t%C3%A9_de_Markov)\n",
    "\n",
    "![proba](./img/proba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markov Reward Process \n",
    "\n",
    "> Une **transition** ici est le passage d'un état à un autre   \n",
    "> Un **épisode** est une suite de transition (exemple: wake up => code => deploy => sleep) \n",
    "\n",
    "Maintenant que nous avons la base, introduisons le deuxième élément, j'ai nommé **les récompenses**. Nous allons ajouter une récompense (Reward) à nos **transitions**. Les récompenses sont des nombres et peuvent être positif ou négatif, ce que nous tentons de faire, c'est de maximiser le nombre de récompenses acquis lors des **épisodes**. \n",
    "\n",
    "Pour chaque épisode, il y a une notion de rendement G, qui est le total des récompenses. L'objectif est de **maximiser** ce retour.\n",
    "\n",
    "![return](./img/return.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un élément important ici est ce qu'on appelle le **discount factor** généralement noté γ (gamma), c'est un nombre compris entre 0 et 1. C'est un peu compliqué, mais restez attentif et ouvert d'esprit : ) c'est important. \n",
    "\n",
    "Essayons de comprendre le calcul ci-dessus.  \n",
    "Pour chaque transition de notre **State Space**, nous avons une récompense que nous récoltons dans l'état t + 1 d'arrivé. Prenons un exemple avec un nouveau système plus proche de la vie réelle.\n",
    "\n",
    "![reward](./img/reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque élément de notre épisode, nous calculons le **rendement** comme étant une somme des récompenses futures. \n",
    "\n",
    "Prenons un exemple avec cette épisode (wake up => code => deploy => sleep) : étant donné notre épisode commençant par l'état wake up, quelle est le rendement possible en étant dans cet état ?  \n",
    "\n",
    "Autrement dit nous allons faire la somme des récompenses de tous les états de cet épisode, soit : -3 + 10 + 3 = 10. \n",
    "\n",
    "On vient donc de calculer le rendement de wake up pour cet épisode. \n",
    "\n",
    "Maintenant, il manque quelque chose. Les récompenses qui arrivent dans le futur vont être multipliées par le **discount factor** (gamma) élevé à la puissance t comme ci-dessous.  \n",
    "\n",
    "* Si gamma est égal à 1, le rendement sera égal à la somme de toutes les récompenses futures de l'épisode, ce qui correspond à une visibilité parfaite de toutes les récompenses futures de l'épisode. \n",
    "\n",
    "* Si gamma est égal à 0, alors le rendement sera égal à la récompense immédiate sans aucunes récompenses futures et cela correspond donc à une visibilité très réduite sur le futur, soit une sorte de myopie. \n",
    "\n",
    "Le gamma est un paramètre très important en RL. Pensez-y comme une mesure calculant jusqu'où dans le futur nous regardons pour estimer le rendement. Plus gamma est proche de 1 plus notre vision est lointaine et plus gamma est proche de 0 plus vous êtes myope : ). \n",
    "\n",
    "![return](./img/return.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon, j'espère ne pas vous avoir perdu, vous y êtes presque !  \n",
    "\n",
    "* Actuellement le calcul de ce rendement G n'est pas vraiment utile, car il est défini pour tous les épisodes spécifiques que nous observons. Et donc ce rendement G calculé pour chaque épisode spécifique peut varier d'un épisode a l'autre même pour un même état. \n",
    "  \n",
    "* Cependant si on calcule **l'espérance du rendement** pour chaque état du système (en faisant la moyenne d'énormément d'épisodes) on peut avoir une valeur très utile appelée **value state** qui nous indique à quel point il est bien d'être dans cet état-là.\n",
    "\n",
    "Pour chaque état du système la valeur v(s) est la moyenne des rendements G de cet état sur des épisodes différents. \n",
    "  \n",
    "![value](./img/valuefunction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Markov Decision Process \n",
    "\n",
    "> Ici une **transition** correspond à ceci : (état => action => nouvelle état => récompense)  \n",
    "\n",
    "On arrive sur la dernière enveloppe, je pense que vous avez déjà une petite idée de comment étendre le processus de Markov sous sa forme de processus de decision markovien.  \n",
    "\n",
    "* Premièrement nous allons ajouter un set d'action qui doit être un nombre d'actions fini. C'est ce qu'on appelle **l'Action Space**. \n",
    "\n",
    "* Deuxièmement nous allons relier les actions avec le changement d'état et le gain de récompenses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![markov](./img/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas d'un processus de Markov nous avions une matrice de transition. C'est un tableau à 2 dimensions qui contient les **probabilités réelles** de passer d'un état 1 à un état 2 dans le système. Petit exemple avec le schéma ci-dessous. La probabilité d'être dans l'état s1 et de passer à l'état s2 est de p12. \n",
    "\n",
    "![process](./img/matrice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons rajouté les actions, il faut que l'on conditionne notre matrice de transitions pour rajouter les actions. On va donc passer d'une matrice à 2 dimensions à une matrice à 3 dimensions.\n",
    "\n",
    "Nous n'observons plus passivement le système, nous pouvons activement choisir de prendre une action à chaque changement d'état. \n",
    "\n",
    "Pour chaque état t, nous avons une matrice où la dimension représentant la profondeur représente les actions que l'on peut prendre et les deux autres dimensions représentent l'état source et l'état d'arrivé. Ci-dessous un schéma récapitulatif. \n",
    "\n",
    "![mdp_pro](./img/mdp_rpo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les actions que l'on prend peuvent maintenant influencer la probabilité de tomber dans l'état de destination. \n",
    "\n",
    "On pourrait se demander pourquoi les actions ne font pas tomber dans l'état de destination avec une probabilité de 100 %. \n",
    "\n",
    "Reprenons l'exemple avec le robot de tout à l'heure. Ce robot peut avoir des moteurs défectueux, et lorsque le robot prend une action il y a une probabilité qu'un de ses moteurs vrille et qu'il tombe dans la case qui est à côté. Si on demande au robot d'aller en haut, il y a donc 90 % de chance qu'il tombe sur la case d'en haut. Ainsi que 10 % de chance qu'il ne bouge pas ou qu'il tombe sur une autre case. Dans la vie, les actions que l'on fait n'aboutissent pas forcément à ce qu'on espérait réellement. Il peut y avoir des imprévus. \n",
    "\n",
    "La récompense que l'on obtient dépend maintenant de l'action qui emmène vers l'état cible et plus seulement de l'état source. \n",
    "\n",
    "C'est similaire à la vie réelle. Lorsque l'on met des efforts dans quelque chose, on gagne généralement de l'expérience même si les résultats de nos efforts ne sont pas forcément formidables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bravo** d'avoir tenu jusqu'ici. On a couvert le plus important pour le RL et le MDP. La dernière chose dont nous devons parler rapidement est de la **Policy**. Ensuite, vous aurez un vocabulaire complet et vous ne passerez plus pour des intrus lors de discussions sur le RL : ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Kaneky !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Après la théorie la pratique !** Ce premier Workshop a introduit une grande partie théorique, mais cela va permettre de ne pas le refaire sur les prochains Workshops. Dans ce premier pas vers la pratique, nous allons travailler sur un exemple très basique pour ne pas vous assommer d'un coup. Puis au fur et à mesure de la série des Workshops, nous complexifierons.\n",
    "\n",
    "Ici, nous allons essayer d'utiliser la **Value Function** pour résoudre un labyrinthe. Il y aura un agent qui va se balader dans un labyrinthe où il y aura une Target à aller chercher. La Target rapporte une récompense de 10 et le feu un malus de 5. L'agent va se balader des milliers de fois dans le labyrinthe et apprendre petit à petit. Il y aura un dump terminal pour que vous puissiez voir le labyrinthe et le dump des valeurs de tous les états (V(s)) s'ajustera en temps réel. Vous comprendrez donc que la **Value Function** représente le **cerveau** de l'agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer certains Tools qui nous seront utiles ainsi que l'environnement. \n",
    "\n",
    "**PS: Respectez bien le type de retour de chaque fonction. Un array d'une case contenant un int n'est pas la même chose qu'un int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "from Envi import Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Première tâche : définir les constantes ainsi qu'un **Namedtuple** représentant les transitions (state, action, next_state, reward). Appelez ce **Namedtuple \"Transition\"** et nous l'utiliserons comme un type nommé semblable à un vecteur de size 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = # Define namedtuple\n",
    "BATCH_SIZE = # Define batch size\n",
    "GAMMA = # Define gamma\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Généralement en RL, l'agent doit posséder une **mémoire**. Cette mémoire va **stocker plusieurs transitions passées**. Ainsi, même si l'agent vient d'effectuer une transition, il l'aura en mémoire encore un bout de temps et continuera de mettre à jour la **Value Function** avec cette transition. Cela permet de s'entraîner plusieurs fois sur des transitions que l'on ne verrait que **très peu de fois**. \n",
    "\n",
    "On va donc créer une classe memory.\n",
    "\n",
    "Initialisez cette classe avec :\n",
    "\n",
    "* Une variable **capacity** représentant la capacité de stockage de transition de la mémoire (quand elle est pleine, la mémoire va oublier les transitions les plus anciennes).\n",
    "\n",
    "* Une variable memory représentant un tableau de transitions (la mémoire en elle-même) \n",
    "\n",
    "* Une variable position représentant l'offset de la mémoire, autrement dit l'index dans le tableau de la dernière transition ajoutée. Cela va nous permettre de rajouter des transitions au bon endroit et de retirer les plus anciennes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        # Need some code\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory: list = []\n",
    "        self.position: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_memory : Memory = Memory(1000)\n",
    "\n",
    "assert (test_memory.capacity == 1000)\n",
    "assert (test_memory.memory == [])\n",
    "assert (test_memory.position == 0)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous allons créer une fonction push. Nous la lierons à la classe memory plus tard. Cette fonction va prendre un *args : Liste en paramètre représentant les paramètres pour créer notre transition (param1 = stata, param 2 = next_state, etc ...). Nous commençons par vérifier si la taille de la mémoire est inférieure à la capacité. Si c'est le cas, on ajoute un élément \"None\" à la mémoire qui représente une case vide.   \n",
    "\n",
    "* On se place dans la mémoire à l'index de la position grâce à notre variable interne et on ajoute à cet endroit une nouvelle transition qui prend en paramètre *args, ce qui va décomposer notre liste d'arguments variadiques en liste et créer la transition, puis la stocker dans la mémoire.   \n",
    "\n",
    "* On update la variable position avec la règle suivante :\n",
    "\n",
    "* On ajoute 1 à la position. Si après l'ajout la position est supérieure à la capacité, la position revient à 0. Pour cela, vous pouvez utiliser un modulo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(self, *args: list):\n",
    "    \"\"\"push a transition\"\"\"\n",
    "    # Need some code\n",
    "\n",
    "setattr(Memory, 'push', push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy_state = [43]\n",
    "dumy_action = [3]\n",
    "dumy_next_state = [44]\n",
    "dumy_reward = [10]\n",
    "\n",
    "dumy1_state = [28]\n",
    "dumy1_action = [3]\n",
    "dumy1_next_state = [44]\n",
    "dumy1_reward = [-5]\n",
    "\n",
    "\n",
    "test_memory : Memory = Memory(2)\n",
    "test_memory.clear()\n",
    "test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward)\n",
    "\n",
    "assert (test_memory.position == 1)\n",
    "assert (test_memory.memory == [Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "\n",
    "test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward)\n",
    "assert (test_memory.position == 0)\n",
    "assert (test_memory.memory == [Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward), Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "\n",
    "test_memory.push(dumy1_state, dumy1_action, dumy1_next_state, dumy1_reward)\n",
    "assert (test_memory.position == 1)\n",
    "assert (test_memory.memory == [Transition(dumy1_state, dumy1_action, dumy1_next_state, dumy1_reward), Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentons deux autres fonctions qui nous serons utile. \n",
    "La fonction batch qui va nous renvoyer un **Sample** de n transitions présent au hasard dans la mémoire de l'agent. Cela va nous permettre d'update la valeur des états avec ce qui est présent dans la mémoire de l'agent. Pour cela, utilisez **random.sample()**. \n",
    "La fonction **len** qui va nous renvoyez la taille actuelle de la mémoire. \n",
    "\n",
    "\n",
    "Ces fonctions seront liées à la classe memory grace à **setattr**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(self, batch_size):\n",
    "    return # Need some code\n",
    "\n",
    "def __len__(self):\n",
    "    return # Need some code\n",
    "\n",
    "setattr(Memory, '__len__', __len__)\n",
    "setattr(Memory, 'batch', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy_state = [43]\n",
    "dumy_action = [3]\n",
    "dumy_next_state = [44]\n",
    "dumy_reward = [10]\n",
    "\n",
    "test_memory.capacity = 20\n",
    "test_memory.clear()\n",
    "[test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward) for i in range(20)]\n",
    "\n",
    "dumy_batch = test_memory.batch(10)\n",
    "\n",
    "assert (len(dumy_batch) == 10)\n",
    "for i in range(10) :\n",
    "     assert(dumy_batch[i].state == [43] and dumy_batch[i].action == [3] and dumy_batch[i].next_state == [44] and dumy_batch[i].reward == [10])\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, attaquons-nous à la classe principale : la classe Agent.  \n",
    "\n",
    "* Cette classe prend en paramètre d'initialisation **l'instance de l'environnement**. \n",
    "\n",
    "* On crée une variable **memory** qu'on initialise avec la class Memory. On veut une mémoire avec une capacité de 2000 pour le moment.\n",
    "\n",
    "* On crée une variable **env** pour stocker l'environnement reçu. \n",
    "\n",
    "* On crée une variable V et on l'initialise avec autant de 0 que l'environnement a de case (pour cela, on utilise la fonction **get_map()** de l'environnement) \n",
    "\n",
    "* Ensuite faite un appel à la fonction **self.init_v()**, que l'on créera plus tard, pour initialiser les valeurs des états o se trouve les **Targets** et les feux avec leur valeur par défaut. Ces valeurs seront égales à la récompense que donnent ces cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: Env):\n",
    "        # Need some code\n",
    "        self.init_v()\n",
    "\n",
    "    def init_v(self):\n",
    "        map = env.get_map()\n",
    "        for j in range(len(map)):\n",
    "            if map[j] == 'T':\n",
    "                self.V[j] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "test_agent : Agent = Agent(env)\n",
    "\n",
    "assert (test_agent.memory.capacity == 2000)\n",
    "assert (len(test_agent.V) == len(env.get_map()))\n",
    "assert (test_agent.env != None)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici quelques fonctions utilitaires pour initialiser notre value table pour chaque case de l'environnement. Et deux fonctions pour **Save** et **Load** la valeur des états. \n",
    "\n",
    "Toutes ces fonctions seront liées à la classe Agent grace à **setattr**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_v(self, path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        for v_f in self.V:\n",
    "            f.write(str(round(v_f, 4)) + \" \")\n",
    "\n",
    "def load_v(self, path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        self.V = f.read().split(\" \")\n",
    "        del self.V[len(self.V) - 1]\n",
    "        for k in range(len(self.V)):\n",
    "            self.V[k] = float(self.V[k])\n",
    "\n",
    "setattr(Agent, 'save_v', save_v) \n",
    "setattr(Agent, 'load_v', load_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la vie réelle, quand nous avons pris l'habitude de faire quelque chose, nous le refaisons encore et encore de la même manière. Mais quelques fois, par hasardn nous divergeons. Prenons un exemple : je suis un élève qui sort de l'école et pour rentrer chez moi, je prends toujours le même chemin, car c'est celui-ci que l'on m'a appris. \n",
    "\n",
    "Un beau jour, je me trompe de chemin et je découvre un chemin plus rapide pour rentrer chez moi. Depuis, j'utilise celui-ci. \n",
    "\n",
    "\n",
    "Et voilà ! Vous venez de voir le concept d'exploration et d'exploitation. Même si notre agent a appris quelque chose, il faut toujours qu'il y ait un pourcentage de chance qu'il prenne des actions aléatoires pour peut-être découvrir de nouvelles choses. Le pourcentage de chance de prendre des actions aléatoires est ce qu'on appelle **l'epsilon**. Au début **epsilon** est très haut, car l'agent doit apprendre. Mais au fur et à mesure, l'epsilon diminue et l'agent prend des actions en fonction de son cerveau et de ce qu'il a appris (soit la value fonction). \n",
    "\n",
    "\n",
    "Ici, nous allons implémenter la fonction **greedy_step**. C'est la fonction qui va prendre les décisions en fonction de ce que l'agent a appris. \n",
    "\n",
    "  \n",
    "\n",
    "* On commence par créer un tableau d'actions. Il y a quatre actions possibles : haut, bas, gauche, droite (1,2,3,4). On stock ce tableau dans une variable nommée actions.\n",
    "\n",
    "* Maintenant, réfléchissons. L'agent est à une position t. On veut savoir quelle action prendre en fonction de ce que l'on a appris. On sait que la value fonction nous permet de savoir à quel point c'est bien d'être dans un état donné. Dans l'environnement que j'ai fait pour cet exercice, je vous ai créé une fonction **predict(action)** qui vous retourne un tableau à un seul élément qui représente la case sur laquelle l'agent se retrouverait s'il prenait cette action. \n",
    "\n",
    "* On sait également que chaque état possède sa **Value Function**. On va donc itérer sur toutes les actions et regarder sur quel état nous tombons. Puis pour chaque état sur lequel nous pourrons tomber, nous allons regarder la valeur de cet état grâce à **self.V(S)**. Ensuite nous allons prendre la valeur maximum trouvée. Enfin nous allons choisir l'action qui nous amène à cet état et le retourner (cf. la valeur de retour de la fonction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_step(self) -> int:\n",
    "    actions = [1, 2, 3, 4]\n",
    "    # Need some Code\n",
    "    # Tips: Predict takes an action and return [next_state of agent] \n",
    "    # Tips : self.V is an array with value for all state so self.V[state] return the value function for that state\n",
    "    return # Return Action (an int between 1 and 4)\n",
    "\n",
    "setattr(Agent, 'greedy_step', greedy_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "\n",
    "test_agent.load_v(\"test.txt\")\n",
    "assert(test_agent.greedy_step() == 2)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous avons la fonction générique qui va prendre **les actions**. En paramètre elle reçoit l'état actuel de l'agent et epsilon. \n",
    "\n",
    "\n",
    "* On va prendre un nombre entre 0 et 1. Si ce nombre est inférieur à **epsilon**, alors on renvoie un nombre entre 1 et 4 pour prendre une action aléatoire. \n",
    "\n",
    "* Si jamais le nombre est supérieur à epsilon, on appele notre fonction **greedy_step**, le wrap dans un tableau à un élément et le retourner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(self, eps_t: int) -> [int]:\n",
    "    # Need some code (if .... return/ else .... return WARNING: cf return type)\n",
    "    # Tips second line : Need to use greedy_step function that return an int between 1 and 4 and wrap that int into an array ([int])\n",
    "    # Tips first line : random.randint/random.uniform \n",
    "\n",
    "setattr(Agent, 'take_action', take_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "\n",
    "test_agent.load_v(\"test.txt\")\n",
    "assert(test_agent.take_action(0) == [2])\n",
    "assert(test_agent.take_action(1)[0] >= 1 and test_agent.take_action(1)[0] <= 4)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rappel pour la suite :  \n",
    "Afin d'acquérir des récompenses, la fonction de valeur est un moyen efficace pour déterminer la valeur d'être dans un état. On note cette fonction V(s). Elle mesure les futures récompenses que nous pourrions obtenir en étant dans cet état s.\n",
    "![rapp](./img/rapp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On se rappelle le calcul du rendement pour un état d'un épisode.\n",
    "\n",
    ">$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma ^ 2 R_{t+3} + ...$  \n",
    "\n",
    "  \n",
    "\n",
    "> On se rappelle également que pour avoir la valeur de l'état, on prend l'espérance de tous les rendements de cet état sur énormément d'épisodes différents.  \n",
    "\n",
    ">$V(s) = \\mathbb{E}[G_t | S_t = s]$  \n",
    "\n",
    "  \n",
    "\n",
    "> On traduit l'expression. On update la valeur de chaque état grâce à cette formule qui reprend ce qu'on a dit juste au-dessus. On prend la valeur actuelle et on lui rajoute la récompense à t + 1 + gamma * la différence de la valeur a l'état t + 2 et l'état actuel.   \n",
    "\n",
    "> On répète cette opération autant de fois que l'on a de transitions, ce qui va ajuster la valeur de l'état et donc simuler l'Espérance du rendement total pour cette état, comme le montre la formule ci dessus.\n",
    "\n",
    ">$V(s) = V(s) + r + \\gamma * [V(s') - V(s)] $ \n",
    "\n",
    "  \n",
    "\n",
    "> Lorsque l'on veutupdate les paramètres d'apprentissage d'une IA, on utilise le **Learning rate** (taux d'apprentissage). Il permet d'ajuster petit à petit la valeur de l'état plutôt que de faire de gros ajustements qui seraient instables.\n",
    "\n",
    ">$V(s) = V(s) + \\alpha * [r + \\gamma * [V(s') - V(s)]] $ \n",
    "\n",
    "\n",
    "> r = reward  \n",
    "> s = state  \n",
    "> s' = next_state  \n",
    "> $\\gamma$ = gamma  \n",
    "> $\\alpha$ = Learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tâche : Implémentez la fonction **learn**. C'est la fonction principale responsable de l'update des **Values Functions** de chaque état.    \n",
    "\n",
    "* Récupérez des transitions de la mémoire de l'agent. (Récupérez BATCH_SIZE transitions). Grace à la fonction batch que vous avez implémentée tout à l'heure. Stockez ce que vous avez récupéré dans une variable **\"transitions\".** \n",
    "\n",
    "* Il va falloir faire quelques **prints** et tester ce que vous avez récupéré de la fonction batch. Actuellement, vous avez récupéré un tableau de **Nametuple Transition**, mais nous voulons un seul **Nametuple Transition** formant un tableau. Pour cela, il va falloir l'opérateur * pour passer le tableau de **Nametuple Transition** à la fonction zip pour justement **unzip** ce tableau de **Nametuple Transition** (zip est son propre inverse). \n",
    "\n",
    "* Ensuite il faut réutiliser * sur le retour du zip pour mettre ce retour sous forme de List. Et enfin, on crée un nouveau **Nametuple Transition** avec cette liste pour créer une seule transition formant un tuple de 4 éléments ou chaque élément est un tableau et on stocke ce **Nametuple Transition** dans une variable appelée **batch**. \n",
    "\n",
    "* **Faites des prints** pour voir ce que vous avez fait. Normalement, vous pouvez accéder à tous les états avec batch.state puis **batch.reward** pour les **rewards** etc... Qui doivent être 4 tableaux. \n",
    "\n",
    "* Il faut maintenant itérer sur **batche.state**, **batch.next_state** et **batch.reward** (vous pouvez faire une boucle for avec un zip et récupérer un élément à chaque tour de boucle) \n",
    "\n",
    "* Ensuite, comme vous l'avez remarqué, chaque unique state est un tableau à 1 élément donc dans la boucle for, initialisez trois variable **s, s_prime et reward** avec l'index 0 de chaque élément respectif (state, next_state et reward). \n",
    "\n",
    "* Une fois que vous avez cela, implémentez l'update de **self.V[s]** avec la formule ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self):\n",
    "    transitions = self.memory.batch(BATCH_SIZE)\n",
    "    #On passe d'un tableau de transition a une transition de tableau\n",
    "    #On met * pour passer l'arg à la fonction zip puis * pour unzip sous forme de list\n",
    "    batch: Transition = Transition(*zip(*transitions))\n",
    "    for state, next_state, reward in zip(batch.state, batch.next_state, batch.reward):\n",
    "        # Need one line of code (look above there is a formula with v[s] = ....)\n",
    "\n",
    "setattr(Agent, 'learn', learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isTired = False\n",
    "assert (isTired)\n",
    "print(\"\\n\\033[92mVery good ! go to last step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voici la dernière fonction : la fonction **main**.  \n",
    "\n",
    "* On commence par créer une instance de l'environnement en lui passant le fichier txt représentant le labyrinthe. \n",
    "\n",
    "* On crée une instance de l'agent en lui donnant l'environnement. \n",
    "\n",
    "* On définit deux variables pour la boucle de jeux et le taux epsilon. \n",
    "\n",
    "* Ensuite on boucle le nombre de fois que l'on veut. \n",
    "\n",
    "* Toute les 10 itérations on diminue l'epsilon.  \n",
    "\n",
    "* On récupère **l'état actuel de l'agent**. (la fonction get_env() de l'env retourne l'état actuel de l'agent) \n",
    "\n",
    "* On demande à l'agent quelle action il veut prendre.  \n",
    "\n",
    "* On passe cette action dans l'environnement grâce à la fonction **step** qui va effectuer l'action \n",
    "\n",
    "* On push la transition que l'agent vient de faire dans sa mémoire \n",
    "\n",
    "* On **render()** pour afficher. \n",
    "\n",
    "* Tant que l'on n'a pas aux moins 128 transitions dans notre mémoire on stack les transitions. \n",
    "\n",
    "* Une fois 128 atteins l'agent va apprendre avec ce qu'il a dans sa mémoire et chaque fois que l'on rajoute une transition dans la mémoire l'agent va en supprimer une autre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Bienvenue dans L-Antique Game n°1 ! Vous allez apprendre toutes sortes de choses sur le Reinforcement Learning !\")\n",
    "    env: Env = Env(\"maze.txt\")\n",
    "    agent: Agent = Agent(env)\n",
    "    i = 0\n",
    "    eps = 0.9\n",
    "    #agent.load_v(\"save.txt\")\n",
    "    while i < 10000:\n",
    "        if i % 10 == 0:\n",
    "            eps = max(eps * 0.9998, 0.05)\n",
    "        state: list = env.get_env()\n",
    "        action: list = agent.take_action(eps)\n",
    "        next_state, reward = env.step(action[0])\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        env.render(agent.V, eps, 0.1)\n",
    "        i += 1\n",
    "        if i > 128:\n",
    "            agent.learn()\n",
    "    agent.save_v(\"save.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}